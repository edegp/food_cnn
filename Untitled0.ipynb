{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edegp/food_cnn/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d1Mkseff_dCA"
      },
      "outputs": [],
      "source": [
        "# %cd ConvNeXt/\n",
        "# !wget https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y15NcX53_yRE"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/facebookresearch/ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iUD4pQPbEBES"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import convnext_tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jVyfrHTrvB4b"
      },
      "outputs": [],
      "source": [
        "demog = pd.read_csv(\"/content/drive/My Drive/foodReward/data/data_demographic_NCNP.csv\")\n",
        "ques = pd.read_csv(\"/content/drive/My Drive/foodReward/data/data_questionnare_NCNP.csv\")\n",
        "resp = pd.read_csv(\"/content/drive/My Drive/foodReward/data/data_responses_NCNP_2types.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8HGSRJe_vWSK"
      },
      "outputs": [],
      "source": [
        "res_L_mean = resp.groupby(\"img\")[\"res_L\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i6soUodZHeYq"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir: str, label_series:pd.Series, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Replace 0 with actual label if available\n",
        "        label = res_L_mean[idx + 1]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define transformations\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(256),\n",
        "#     transforms.CenterCrop(224),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# Load dataset\n",
        "image_dir = '/content/drive/My Drive/foodReward/data/Database'\n",
        "dataset = ImageDataset(image_dir=image_dir,label_series= res_L_mean, transform=None)\n",
        "\n",
        "# Create a data loader\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# # Sample batch\n",
        "# for images, labels in data_loader:\n",
        "#     print(f'Batch of images shape: {images.shape}')\n",
        "#     print(f'Batch of labels shape: {labels.shape}')\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T8KfSBBaIfLT"
      },
      "outputs": [],
      "source": [
        "# Function to visualize a batch of images\n",
        "def show_images(images, labels, n=8):\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(2, n//2, i + 1)\n",
        "        img = images[i].permute(1, 2, 0).numpy()\n",
        "        img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # Unnormalize\n",
        "        img = img.clip(0, 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f'Label: {labels[i].item()}')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e2jJw4OKccyB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert augmented data into a DataLoader-friendly format\n",
        "class AugmentedDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "# Augment the training dataset by creating additional copies with transformations\n",
        "def augment_dataset(original_dataset, num_augmentations):\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    for img, label in original_dataset:\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_img = train_transform(img)\n",
        "            augmented_images.append(augmented_img)\n",
        "            augmented_labels.append(label)\n",
        "    return augmented_images, augmented_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHHjwdVOGUJS",
        "outputId": "d0b45929-ec69-48e4-9655-14e712b789b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
            "100%|██████████| 109M/109M [00:00<00:00, 190MB/s] \n"
          ]
        }
      ],
      "source": [
        "# Define transformations for training and validation datasets\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(20),  # Increased rotation range\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=15),\n",
        "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "    transforms.RandomGrayscale(p=0.25),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define transformations without augmentation for the validation and test sets\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define split sizes\n",
        "train_size = int(0.75 * len(dataset))\n",
        "val_size = int(0.125 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Apply augmentations to create additional data points\n",
        "num_augmentations = 4  # Number of augmented versions per original image\n",
        "augmented_images, augmented_labels = augment_dataset(train_dataset, num_augmentations)\n",
        "\n",
        "# Create an augmented training dataset\n",
        "augmented_train_dataset = AugmentedDataset(augmented_images, augmented_labels)\n",
        "\n",
        "# Combine original and augmented datasets\n",
        "combined_train_dataset = torch.utils.data.ConcatDataset([train_dataset, augmented_train_dataset])\n",
        "\n",
        "# # # Get a batch of images and labels from the data loader\n",
        "# images, labels = next(iter(combined_train_dataset))\n",
        "# show_images(images, labels, n=8)\n",
        "\n",
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform = val_test_transform\n",
        "test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(combined_train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load pre-trained ConvNeXt model\n",
        "model = convnext_tiny(pretrained=True)\n",
        "\n",
        "# Modify the classifier to output a single continuous value for regression\n",
        "model.classifier[2] = nn.Linear(model.classifier[2].in_features, 1)\n",
        "\n",
        "# Loss function and optimizer for regression\n",
        "criterion = nn.HuberLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kAJUal6DcmlE"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning function for regression\n",
        "def fine_tune_regression(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "        # Evaluate the model on validation data\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        print(f'Validation Loss: {total_loss/len(val_loader)}')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Test the model\n",
        "def test_model(model, test_loader, criterion):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_corr = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
        "            outputs = model(inputs)\n",
        "            # calclate correlation coefficient\n",
        "            corr = torch.corrcoef(torch.cat((outputs, labels), dim=1).T)[0, 1]\n",
        "            test_corr += corr.item()\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "    print(f'Test Loss: {test_loss/len(test_loader)}')\n",
        "    print(f'Test Correlation: {test_corr/len(test_loader)}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJOCtd7tcnhR",
        "outputId": "e9f6a282-9858-4792-a0c2-4fa4762330cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.45674435467947094\n",
            "Validation Loss: 0.25228212773799896\n",
            "Epoch 2/20, Loss: 0.24557117706253415\n",
            "Validation Loss: 0.22051187604665756\n",
            "Epoch 3/20, Loss: 0.1705087797272773\n",
            "Validation Loss: 0.1808598805218935\n",
            "Epoch 4/20, Loss: 0.10241540170141629\n",
            "Validation Loss: 0.18433713912963867\n",
            "Epoch 5/20, Loss: 0.05634997845405624\n",
            "Validation Loss: 0.16611464321613312\n",
            "Epoch 6/20, Loss: 0.03562978090984481\n",
            "Validation Loss: 0.16220567747950554\n",
            "Epoch 7/20, Loss: 0.024504553251678034\n",
            "Validation Loss: 0.164857754483819\n",
            "Epoch 8/20, Loss: 0.019590637345044386\n",
            "Validation Loss: 0.1665643583983183\n",
            "Epoch 9/20, Loss: 0.018157830812214386\n",
            "Validation Loss: 0.1628615316003561\n",
            "Epoch 10/20, Loss: 0.017502474940071504\n",
            "Validation Loss: 0.1635803859680891\n",
            "Epoch 11/20, Loss: 0.015684071422687598\n",
            "Validation Loss: 0.17144652642309666\n",
            "Epoch 12/20, Loss: 0.014770094127882095\n",
            "Validation Loss: 0.16942264139652252\n",
            "Epoch 13/20, Loss: 0.013986136821941251\n",
            "Validation Loss: 0.15953294932842255\n",
            "Epoch 14/20, Loss: 0.012833547969126986\n",
            "Validation Loss: 0.16445010527968407\n",
            "Epoch 15/20, Loss: 0.011819635429197834\n",
            "Validation Loss: 0.16007279604673386\n",
            "Epoch 16/20, Loss: 0.01068335554695555\n",
            "Validation Loss: 0.15877131186425686\n",
            "Epoch 17/20, Loss: 0.009636280175653242\n",
            "Validation Loss: 0.17011074721813202\n",
            "Epoch 18/20, Loss: 0.010983330091195447\n",
            "Validation Loss: 0.16024786233901978\n",
            "Epoch 19/20, Loss: 0.010713255675953059\n",
            "Validation Loss: 0.1588673535734415\n",
            "Epoch 20/20, Loss: 0.010710889353815999\n",
            "Validation Loss: 0.157730957493186\n"
          ]
        }
      ],
      "source": [
        "# 相関係数をlossにする\n",
        "# Fine-tune the model for regression\n",
        "model = fine_tune_regression(model, train_loader, val_loader, criterion, optimizer, num_epochs=20)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/foodReward/model/convnext_tiny_regression.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-jRIQXpfcrH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4bf7a7-6804-49da-9b53-5178052cdc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1651686578989029\n",
            "Test Correlation: 0.6612539812922478\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "model = test_model(model, test_loader, criterion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "mount_file_id": "12FfhUh8pg29sE3X9AC3sXKR7sYKjAcDT",
      "authorship_tag": "ABX9TyPAEERTdegsAZQqMt1l7itn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}